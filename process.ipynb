{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割节点和边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "split link\n",
    "'''\n",
    "if not os.path.exists(\"splitEdge\"):\n",
    "    os.mkdir(\"splitEdge\")\n",
    "\n",
    "df = pd.read_csv('./data/Link.csv')\n",
    "\n",
    "for key, group in tqdm(df.groupby(\"relation\")):\n",
    "\n",
    "    # group = group.drop(columns='relation')\n",
    "    group.reset_index(drop=True, inplace=True)\n",
    "    group.rename(columns={\"relation\": \":TYPE\",\n",
    "                          \"source\": \":START_ID\",\n",
    "                          \"target\": \":END_ID\"},  inplace=True)\n",
    "\n",
    "    if key in [\"r_cert\", \"r_subdomain\", \"r_request_jump\", \"r_dns_a\"]:\n",
    "        group.insert(group.shape[1], \"weight:int\", 2)\n",
    "    elif key in [\"r_cert_chain\", \"r_cname\"]:\n",
    "        group.insert(group.shape[1], \"weight:int\", 1)\n",
    "    group.to_csv(\n",
    "        './splitEdge/' + key + '.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split node\n",
    "'''\n",
    "# get work Path\n",
    "path = os.getcwd()\n",
    "# read csv file\n",
    "dataFrame = pd.read_csv(path+\"\\\\data\\\\Node.csv\")\n",
    "community = pd.read_csv('./data/node-community.csv')\n",
    "dataFrame = pd.merge(dataFrame, community, left_on='id',\n",
    "                right_on='id', how='left')\n",
    "del community\n",
    "# group\n",
    "groups = dataFrame.groupby(\"type\")\n",
    "\n",
    "Domain_gp = groups.get_group(\"Domain\")\n",
    "IP_gp = groups.get_group(\"IP\")\n",
    "Cert_gp = groups.get_group(\"Cert\")\n",
    "Register_Name_gp = groups.get_group(\"Whois_Name\")\n",
    "Register_Email_gp = groups.get_group(\"Whois_Email\")\n",
    "Register_Phone_gp = groups.get_group(\"Whois_Phone\")\n",
    "IPC_gp = groups.get_group(\"IP_C\")\n",
    "ASN_gp = groups.get_group(\"ASN\")\n",
    "\n",
    "# split DF\n",
    "Domain = Domain_gp.set_index(\"id\")\n",
    "IP = IP_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "Cert = Cert_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "Register_Name = Register_Name_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "Register_Email = Register_Email_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "Register_Phone = Register_Phone_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "IPC = IPC_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "ASN = ASN_gp.drop(\"industry\", axis=1).set_index(\"id\")\n",
    "\n",
    "if not os.path.exists(\"splitNode\"):\n",
    "    os.mkdir(\"splitNode\")\n",
    "Domain.to_csv(path+\"\\\\splitNode\\\\Domain.csv\", encoding='utf_8_sig')\n",
    "print(\"Domain finished\")\n",
    "IP.to_csv(path+\"\\\\splitNode\\\\IP.csv\", encoding='utf_8_sig')\n",
    "print(\"IP finished\")\n",
    "Cert.to_csv(path+\"\\\\splitNode\\\\Cert.csv\", encoding='utf_8_sig')\n",
    "print(\"Domain finished\")\n",
    "Register_Name.drop(columns=[\"community\"], inplace=True)\n",
    "Register_Name.to_csv(path+\"\\\\splitNode\\\\Register_Name.csv\",\n",
    "                     encoding='utf_8_sig')\n",
    "print(\"Register_Name finished\")\n",
    "Register_Email.drop(columns=[\"community\"], inplace=True)\n",
    "Register_Email.to_csv(\n",
    "    path+\"\\\\splitNode\\\\Register_Email.csv\", encoding='utf_8_sig')\n",
    "print(\"Register_Email finished\")\n",
    "Register_Phone.drop(columns=[\"community\"], inplace=True)\n",
    "Register_Phone.to_csv(\n",
    "    path+\"\\\\splitNode\\\\Register_Phone.csv\", encoding='utf_8_sig')\n",
    "print(\"Register_Phone finished\")\n",
    "IPC.drop(columns=[\"community\"], inplace=True)\n",
    "IPC.to_csv(path+\"\\\\splitNode\\\\IPC.csv\", encoding='utf_8_sig')\n",
    "print(\"IPC finished\")\n",
    "ASN.drop(columns=[\"community\"], inplace=True)\n",
    "ASN.to_csv(path+\"\\\\splitNode\\\\ASN.csv\", encoding='utf_8_sig')\n",
    "print(\"ASN finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并节点，重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import case\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"split industry\")\n",
    "\n",
    "dict_map = {\n",
    "    \"A\": \"porn\",\n",
    "    \"B\": \"gambling\",\n",
    "    \"C\": \"fraud\",\n",
    "    \"D\": \"drug\",\n",
    "    \"E\": \"gun\",\n",
    "    \"F\": \"hacker\",\n",
    "    \"G\": \"trading\",\n",
    "    \"H\": \"pay\",\n",
    "    \"I\": \"other\"\n",
    "}\n",
    "nodepath = './splitNode/'\n",
    "edgepath = './splitEdge/'\n",
    "saveNodePath = './processedData/Node/'\n",
    "saveEdgePath = './processedData/Edge/'\n",
    "if not os.path.exists(saveNodePath):\n",
    "    os.makedirs(saveNodePath)\n",
    "if not os.path.exists(saveEdgePath):\n",
    "    os.makedirs(saveEdgePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将email，phone，name作为属性添加到domain上，并将industry拆分成7种犯罪属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge domain and (email, name, phone)\n",
    "Split industry into 9 attributes\n",
    "'''\n",
    "def apply_split(row):\n",
    "    res = re.findall('[A-Z]', row['industry'])\n",
    "    for s in res:\n",
    "        row[dict_map[s]] = True\n",
    "    row[\"weight:int\"] = len(res)\n",
    "    return row\n",
    "\n",
    "df = pd.read_csv(nodepath + 'Domain.csv')\n",
    "\n",
    "# merge domain & email\n",
    "df_whois_Email = pd.read_csv(edgepath + 'r_whois_email.csv')\n",
    "df_whois_Email.drop_duplicates(\n",
    "    subset=[':START_ID'], keep='first', inplace=True)\n",
    "df_Email = pd.read_csv(nodepath + 'Register_Email.csv')\n",
    "\n",
    "df = pd.merge(df, df_whois_Email, left_on='id',\n",
    "              right_on=':START_ID', how='left')\n",
    "df = pd.merge(df, df_Email, left_on=':END_ID', right_on='id', how='left')\n",
    "\n",
    "df = df.drop(columns=[\":TYPE\", \":START_ID\", \":END_ID\", \"type_y\"])\n",
    "df.rename(columns={\"id_x\": \"id\",\n",
    "                   \"id_y\": \"email_id\",\n",
    "                   \"name_x\": \"name\",\n",
    "                   \"type_x\": \"type\",\n",
    "                   \"name_y\": \"email\",\n",
    "                   \"community\": \"community:int\"},  inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "# merge domain & name\n",
    "df_whois_Name = pd.read_csv(edgepath + 'r_whois_name.csv')\n",
    "df_whois_Name.drop_duplicates(subset=[':START_ID'], keep='first', inplace=True)\n",
    "df_Name = pd.read_csv(nodepath + 'Register_Name.csv')\n",
    "\n",
    "df = pd.merge(df, df_whois_Name, left_on='id',\n",
    "              right_on=':START_ID', how='left')\n",
    "df = pd.merge(df, df_Name, left_on=':END_ID', right_on='id', how='left')\n",
    "\n",
    "df = df.drop(columns=[\":TYPE\", \":START_ID\", \":END_ID\", \"type_y\"])\n",
    "df.rename(columns={\"id_x\": \"id\",\n",
    "                   \"id_y\": \"register_id\",\n",
    "                   \"name_x\": \"name\",\n",
    "                   \"type_x\": \"type\",\n",
    "                   \"name_y\": \"register\"},  inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "# merge domain & phone\n",
    "df_whois_Phone = pd.read_csv(edgepath + 'r_whois_Phone.csv')\n",
    "df_whois_Phone.drop_duplicates(\n",
    "    subset=[':START_ID'], keep='first', inplace=True)\n",
    "df_Phone = pd.read_csv(nodepath + 'Register_Phone.csv')\n",
    "\n",
    "df = pd.merge(df, df_whois_Phone, left_on='id',\n",
    "              right_on=':START_ID', how='left')\n",
    "df = pd.merge(df, df_Phone, left_on=':END_ID', right_on='id', how='left')\n",
    "\n",
    "df = df.drop(columns=[\":TYPE\", \":START_ID\", \":END_ID\", \"type_y\"])\n",
    "df.rename(columns={\"id_x\": \"id:ID\",\n",
    "                   \"id_y\": \"phone_id\",\n",
    "                   \"name_x\": \"name\",\n",
    "                   \"type_x\": \":LABEL\",\n",
    "                   \"name_y\": \"phone\"},  inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "for insert_c in list(dict_map.values()):\n",
    "    df.insert(df.shape[1], insert_c, False)\n",
    "\n",
    "df.insert(df.shape[1], \"weight:int\", 0)\n",
    "\n",
    "# df.apply的tqdm写法，用来显示进度条\n",
    "df = df.progress_apply(apply_split, axis=1)\n",
    "df = df.drop(columns=\"industry\")\n",
    "df[\"community:int\"] = df[\"community:int\"].astype(int)\n",
    "df.to_csv(saveNodePath + 'Domain.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将asn，ipc作为属性添加到IP上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merge IP and (asn, cidr)\n",
    "'''\n",
    "df = pd.read_csv(nodepath + 'IP.csv')\n",
    "\n",
    "# merge IP & ASN\n",
    "df_r_asn = pd.read_csv(edgepath + 'r_asn.csv')\n",
    "df_r_asn.drop_duplicates(subset=[':START_ID'], keep='first', inplace=True)\n",
    "df_ASN = pd.read_csv(nodepath + 'ASN.csv')\n",
    "\n",
    "df = pd.merge(df, df_r_asn, left_on='id', right_on=':START_ID', how='left')\n",
    "df = pd.merge(df, df_ASN, left_on=':END_ID', right_on='id', how='left')\n",
    "\n",
    "\n",
    "df = df.drop(columns=[\":TYPE\", \":START_ID\", \":END_ID\", \"type_y\"])\n",
    "df.rename(columns={\"id_x\": \"id\",\n",
    "                   \"id_y\": \"asn_id\",\n",
    "                   \"name_x\": \"name\",\n",
    "                   \"type_x\": \"type\",\n",
    "                   \"name_y\": \"asn\",\n",
    "                   \"community\": \"community:int\"},  inplace=True)\n",
    "print(df.columns)\n",
    "\n",
    "# merge IP & IPC\n",
    "df_cidr = pd.read_csv(edgepath + 'r_cidr.csv')\n",
    "df_cidr.drop_duplicates(subset=[':START_ID'], keep='first', inplace=True)\n",
    "df_IPC = pd.read_csv(nodepath + 'IPC.csv')\n",
    "\n",
    "df = pd.merge(df, df_cidr, left_on='id', right_on=':START_ID', how='left')\n",
    "df = pd.merge(df, df_IPC, left_on=':END_ID', right_on='id', how='left')\n",
    "\n",
    "\n",
    "df = df.drop(columns=[\":TYPE\", \":START_ID\", \":END_ID\", \"type_y\"])\n",
    "df.rename(columns={\"id_x\": \"id:ID\",\n",
    "                   \"id_y\": \"ipc_id\",\n",
    "                   \"name_x\": \"name\",\n",
    "                   \"type_x\": \":LABEL\",\n",
    "                   \"name_y\": \"ipc\"},  inplace=True)\n",
    "print(df.columns)\n",
    "df[\"community:int\"] = df[\"community:int\"].astype(int)\n",
    "df.to_csv(saveNodePath + 'IP.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重命名cert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rename Cert\n",
    "'''\n",
    "df_cert = pd.read_csv(nodepath + 'Cert.csv')\n",
    "df_cert.rename(columns={\"id\": \"id:ID\",\n",
    "                        \"type\": \":LABEL\",\n",
    "                        \"community\": \"community:int\"},  inplace=True)\n",
    "print(df_cert.columns)\n",
    "df_cert[\"community:int\"] = df_cert[\"community:int\"].astype(int)\n",
    "df_cert.to_csv(saveNodePath + 'Cert.csv',\n",
    "               index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重命名link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select Edge\n",
    "'''\n",
    "splitEdge = os.listdir(edgepath)\n",
    "for file in tqdm(splitEdge):\n",
    "    if file == 'r_cert_chain.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'cert-cert.csv')\n",
    "    elif file == 'r_cert.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'domain-cert.csv')\n",
    "    elif file == 'r_cname.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'domain-cname.csv')\n",
    "    elif file == 'r_dns_a.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'domain-IP.csv')\n",
    "    elif file == 'r_request_jump.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'domain-domain.csv')\n",
    "    elif file == 'r_subdomain.csv':\n",
    "        copyfile(edgepath + file, saveEdgePath + 'domain-subdomain.csv')\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# （没用）合并社区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "node = pd.read_csv('./data/Node.csv')\n",
    "community = pd.read_csv('./data/node-community.csv')\n",
    "node = pd.merge(node, community, left_on='id',\n",
    "                right_on='id', how='left')\n",
    "# node.drop(columns=[\"name\", \"type\", \"industry\"], inplace=True)\n",
    "del community\n",
    "\n",
    "with open('modify.csv','a+', encoding='utf-8-sig',newline='') as f:#r为标识符，表示只读\n",
    "    writer = csv.writer(f)\n",
    "    # cname = pd.read_csv(\n",
    "    #     './processedData/Edge/domain-cname.csv').drop(columns=[\":TYPE\", \"weight:int\"])\n",
    "    # for i, row in tqdm(cname.iterrows()):\n",
    "    #     c_start = node[node[\"id\"] == row[\":START_ID\"]][\"community\"].values[0]\n",
    "    #     c_end = node[node[\"id\"] == row[\":END_ID\"]][\"community\"].values[0]\n",
    "    #     if(c_start != c_end):\n",
    "    #         writer.writerow([c_end, c_start])\n",
    "    #         node.loc[node[\"community\"] == c_end, [\"community\"]] = c_start\n",
    "        \n",
    "\n",
    "    subdomain = pd.read_csv(\n",
    "        './processedData/Edge/domain-subdomain.csv').drop(columns=[\":TYPE\", \"weight:int\"])\n",
    "    for i, row in tqdm(subdomain.iterrows()):\n",
    "        s_start = node[node[\"id\"] == row[\":START_ID\"]][\"community\"].values[0]\n",
    "        s_end = node[node[\"id\"] == row[\":END_ID\"]][\"community\"].values[0]\n",
    "        if(s_start != s_end):\n",
    "            writer.writerow([s_end, s_start])\n",
    "            node.loc[node[\"community\"] == s_end, [\"community\"]] = s_start\n",
    "        \n",
    "    node.to_csv('CommunityNode.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 社区连接图数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([pd.read_csv('./processedData/Edge/cert-cert.csv', encoding=\"utf-8\"),\n",
    "                pd.read_csv('./processedData/Edge/domain-cert.csv', encoding=\"utf-8\"),\n",
    "                pd.read_csv('./processedData/Edge/domain-cname.csv', encoding=\"utf-8\"),\n",
    "                pd.read_csv('./processedData/Edge/domain-domain.csv', encoding=\"utf-8\"),\n",
    "                pd.read_csv('./processedData/Edge/domain-IP.csv', encoding=\"utf-8\"),\n",
    "                pd.read_csv('./processedData/Edge/domain-subdomain.csv', encoding=\"utf-8\")])\n",
    "df.columns = ['type', 'start', 'end', 'weight']\n",
    "df = pd.DataFrame(df, columns=['start', 'end'])\n",
    "df_Start = pd.DataFrame(df, columns=['start'])\n",
    "df_End = pd.DataFrame(df, columns=['end'])\n",
    "\n",
    "df_community_start = pd.read_csv(\n",
    "    './data/node-community.csv', encoding=\"utf-8\")\n",
    "df_community_end = pd.read_csv('./data/node-community.csv', encoding=\"utf-8\")\n",
    "df_community_start.columns = ['start', 'community']\n",
    "df_community_end.columns = ['end', 'community']\n",
    "\n",
    "start = pd.merge(df_Start, df_community_start, how='left')\n",
    "end = pd.merge(df_End, df_community_end, how='left')\n",
    "start.columns = ['start', 'communityS']\n",
    "end.columns = ['end', 'communityE']\n",
    "result = pd.concat([start, end], axis=1)\n",
    "print(result)\n",
    "SR = []\n",
    "ER = []\n",
    "for index, row in result.iterrows():\n",
    "    if(row['communityS'] != row['communityE']):\n",
    "        SR.append(row['communityS'])\n",
    "        ER.append(row['communityE'])\n",
    "\n",
    "Final = pd.DataFrame({'source': SR,\n",
    "                      'target': ER})\n",
    "dfFinal = Final.drop_duplicates()\n",
    "dfFinal.to_csv('community2community.csv', index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 去掉社区节点数小于等于3的社区连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "c2c = pd.read_csv('./data/community2community.csv')\n",
    "c3 = pd.read_csv('./data/community-nodes-3.csv')\n",
    "c2c = c2c[(c2c.source.isin(c3.communityId.values))&(c2c.target.isin(c3.communityId.values))].reset_index(drop=True)\n",
    "c2c.to_csv('community2community-3.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv转json\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/community2community-3.csv')\n",
    "df.to_json('./app/public/community_link.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "neighbour: 100%|██████████| 37614/37614 [04:04<00:00, 153.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# 社区节点加邻居\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"neighbour\")\n",
    "def fn(row):\n",
    "  res = set()\n",
    "  data = {'id':[row[\"id\"]]}\n",
    "  _ = pd.DataFrame(data)\n",
    "  _ = pd.merge(_, _df, how='left', left_on=\"id\", right_on=\"source\").drop(columns=[\"source\"])\n",
    "  _ = pd.merge(_, _df, how='left', left_on=\"id\", right_on=\"target\").drop(columns=[\"target_y\"]).dropna(axis=1,how='all')\n",
    "  if 'source' in _.columns:\n",
    "    res.update(_[\"source\"].values)\n",
    "  if 'target_x' in _.columns:\n",
    "    res.update(_[\"target_x\"].values)\n",
    "  row[\"neighbour\"] = list(res)\n",
    "  return row\n",
    "df = pd.read_json('./data/community_node.json')\n",
    "_df = pd.read_json('./data/community_link.json')\n",
    "df = df.progress_apply(fn, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_json('./data/community_node1.json', orient=\"records\")\n",
    "# df[df['neighbour'].str.len()!=0].to_json('./data/community_node2.json', orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'key', 'node_num', 'wrong_num', 'porn', 'gambling', 'fraud',\n",
      "       'drug', 'gun', 'hacker', 'trading', 'pay', 'other', 'neighbour',\n",
      "       'wrong_list'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 适配lineup porn,gambling,fraud,drug,gun,hacker,hacker,pay,other\n",
    "df = df[df['neighbour'].str.len()!=0].drop(columns=[\"Domain\", \"Cert\"])\n",
    "df.to_json('./data/community_node2.json', orient=\"records\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'key', 'node_num', 'wrong_num', 'neighbour', 'wrong_list'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df[\"wrong_list\"] = df[[\"porn\",\"gambling\",\"fraud\",\"drug\",\"gun\",\"hacker\",\"trading\",\"pay\",\"other\"]].values.tolist()\n",
    "test = df.drop(columns=[\"porn\",\"gambling\",\"fraud\",\"drug\",\"gun\",\"hacker\",\"trading\",\"pay\",\"other\"], inplace=False)\n",
    "test.to_json('./data/community_node3.json', orient=\"records\")\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b2d9b31d3c403a70b2543a246ce306ec0df2865bb3d11e1e8547e68a4ec51ee"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('ts')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
